---
title: "MovieLensCapstone_JHW"
author: "John Wilson"
date: "8/13/2020"
output: pdf_document
---

```{r setup, include=FALSE}
options(tinytex.verbose = TRUE)
knitr::opts_chunk$set(echo = TRUE)
```
**Executive Summary:**

This report is focused on predicting movie ratings based upon the variables available in the dataset provided. Being able to predict ratings is of benefit both to streaming services and to the actual viewer so that time is not wasted watching something that would not be of interest. Also it allows the streaming service to provide content that lines up better with the viewers' preferences. The data was cleaned up prior to the beginning of the analysis but includes 
10 million ratings for over 10,000 movies utilizing 70,000 users and nearly 800 genres. 10% of this data was reserved for validation of the models that would be formed through out this process. The remaining data was further divided into train and test sets to confirm models prior to validation. 8 models were trained on the data and proved Matrix Factorization to be the most accurate model with a root mean square error of 0.797 on the test set. That model was then applied to the validation set yielding a final RMSE of 0.797 which is below the target of 0.8649.

**Data Preparation:**

Prior to importing the data, the required packages for processing were checked for and if not present were installed then loaded. Then the data was imported from the grouplens organization and was modified into tidy format with the variables of: movieId, userId, title, timestamp, rating and genres. To prevent errors later in the project once the 10% cut of data was designated for validation, it was confirmed that movieId and userId would exist in both data sets in order for models to be applicable across the sets.

```{r include=TRUE, message=FALSE}
# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(recosystem)) install.packages("recosystem", repos = "http://cran.us.r-project.org")
if(!require(knitr)) install.packages("knitr", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(lubridate)
library(recosystem)
library(knitr)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
                                           title = as.character(title),
                                           genres = as.character(genres))
# if using R 4.0 or later
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))

movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
# if using R 3.5 or earlier, use `set.seed(1)` instead
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

```

**Data Review:**

Below, it can be seen the structure of the edx data set that will be used to form and test models prior to validation. Note the variables, their class and the number of observations.
```{r include=TRUE}
str(edx)
```

Prior to performing calculations it is best to know if there are any NA values that cause potential issues. With the code below, it is determined there are none.
```{r include=TRUE}
sum(is.na(edx))
```

To get an idea of unique items within variables the following code was ran. 
```{r include=TRUE, warning=FALSE, message=FALSE}
unique_variables <- data_frame(Variable=c("Movies","Users","Genres"),
           Count=c(length(unique(edx$movieId)),length(unique(edx$userId)),
                   length(unique(edx$genres))))
unique_variables
```

Now with a deeper look into the data, the number of ratings very across movieId, genres, and userId. 3 plots for each of the items were created. To not overload the x axis with labels, histogram bins were used just to display the variation in counts.

```{r count variation, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% count(movieId)%>%ggplot(aes(n)) + 
  geom_histogram(color = "black")+ scale_x_log10() + labs(title="Movies", x="", y="Count")
```

```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% count(genres)%>%ggplot(aes(n)) + 
  geom_histogram(color = "black")+ scale_x_log10() + labs(title="Genres", x="", y="Count")
```

```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% count(userId) %>% ggplot(aes(n)) + 
  geom_histogram(color = "black")+ scale_x_log10() + labs(title="Users", x="", y="Count")
```

Further investigation shows that harsher ratings have been giving over the years with the earlier years having a higher average rating.
```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>% mutate(year = year(timestamp)) %>%
  group_by(year) %>% summarize(avg_rating=mean(rating)) %>%
  ggplot(aes(year,avg_rating)) + geom_smooth()
```

Time of day also appears to play a role in ratings as lower ratings typical are recorded between 5 and 11am. Higher ratings come closer to midnight.
```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>% mutate(hour = hour(timestamp)) %>%
  group_by(hour) %>% summarize(avg_rating=mean(rating)) %>%
  ggplot(aes(hour,avg_rating)) + geom_smooth()
```

Coincidentally perhaps, those same hours are when the least amount of ratiings are being recorded.
```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>% mutate(hour = hour(timestamp)) %>%
  group_by(hour) %>% summarize(Count=n()) %>%
  ggplot(aes(hour,Count)) + geom_smooth() + scale_y_log10()
```

In most households, it is probably expected that the most movies would be watched over the weekend. These charts display how average ratings are impacted by day of the week and it is discovered the most active day for ratings, is actually Wednesday. The middle of the week also seems to yield lower ratiings.
```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>% mutate(days = wday(timestamp)) %>%
  group_by(days) %>% summarize(avg_rating=mean(rating)) %>%
  ggplot(aes(days,avg_rating)) + geom_smooth() + scale_y_log10()
```

```{r, echo=FALSE, fig.align='center', warning=FALSE, message=FALSE}
edx %>% mutate(timestamp = as_datetime(timestamp)) %>% mutate(days = wday(timestamp)) %>%
  group_by(days) %>% summarize(Count=n()) %>%
  ggplot(aes(days,Count)) + geom_smooth() + scale_y_log10()
```

To get an idea of movies included and how they were rated, the 5 best and worse movies were pulled that had more than 100 ratings.

```{r, include=TRUE, warning=FALSE, message=FALSE}
edx %>% group_by(title) %>% summarize(n=n(),avg_rating=mean(rating))%>% 
  filter(n>100) %>% arrange(desc(avg_rating)) %>% head(5)
```

```{r, include=TRUE, warning=FALSE, message=FALSE}
edx %>% group_by(title) %>% summarize(n=n(),avg_rating=mean(rating))%>% 
  filter(n>100) %>% arrange(desc(avg_rating)) %>% tail(5)
```

**Technical Analysis:**

Now that the data has been reviewed and a sense of what is included, the analysis can begin. Per instructions the validation data can only utilized on the final step of testing the best model. Therefore, the edx data set is split into a train and test set to form the models necessary for predictions.
```{r, warning=FALSE, message=FALSE}
set.seed(1, sample.kind = "Rounding")
test_index <- createDataPartition(edx$rating,times=1,p=0.2,list = FALSE)
test_set <- edx[test_index,]
train_set <- edx[-test_index,]
```

To confirm the smooth operation of the models, a quick check to confirm movieId and userId are in both sets.
```{r}
test_set <- test_set %>% semi_join(train_set, by = "movieId") %>% 
  semi_join(train_set, by = "userId")
```

The first model formed was just calculating the average of the movie ratings and applying it across the board. This approach is often called the naive average approach and is a good starting point. In order to be organized throughout this analysis a table called RMSE_result was formed. 
```{r, warning=FALSE, message=FALSE}
mu <- mean(train_set$rating)
avg_pred <- rep(mu,nrow(test_set))
avg_rmse <- RMSE(test_set$rating,avg_pred)
RMSE_result0 <- data_frame(Model = "Naive-Avg", 
                           RMSE = avg_rmse) #create table for results
```

Individuals, are just that individuals. So one viewer could easily consider a movie good while another may not. Also there could be bias based upon the movie itself and certain genres will fair better than others. All of these bias were taken into consideration and models were formed to access these bias by beginning with movieId then progressing with userId and finally genres.
```{r, warning=FALSE, message=FALSE}
# Movie Effect Model
movie_avg <- train_set %>% group_by(movieId) %>% 
  summarize(b_m = mean(rating-mu))
movie_pred <- test_set %>% left_join(movie_avg, by='movieId') %>% 
  mutate(pred=mu+b_m) %>% pull(pred)
movie_rmse <- RMSE(movie_pred,test_set$rating)
RMSE_result1 <- bind_rows(RMSE_result0, 
                          data_frame(Model="Movie Effect Model", RMSE = movie_rmse ))

# Movie+User Effect Model
user_avg <- train_set %>% left_join(movie_avg, by='movieId') %>% 
  group_by(userId) %>% summarize(b_u = mean(rating-mu-b_m))
user_pred <- test_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>%
  mutate(pred=mu+b_m+b_u) %>% pull(pred)
user_rmse <- RMSE(user_pred,test_set$rating)
RMSE_result1 <- bind_rows(RMSE_result1, 
                          data_frame(Model="Movie+User Effect Model", RMSE = user_rmse ))

# Movie+User+Genre Effect Model
genre_avg <- train_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>% group_by(genres) %>% 
  summarize(b_g = mean(rating-mu-b_m-b_u))
genre_pred <- test_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>% left_join(genre_avg, by='genres') %>% 
  mutate(pred=mu+b_m+b_u+b_g) %>% pull(pred)
genre_rmse <- RMSE(genre_pred,test_set$rating)
RMSE_result1 <- bind_rows(RMSE_result1, 
                          data_frame(Model="Movie+User+Genre Model", RMSE = genre_rmse ))

```

To see if there was enough bias in time impact two models were created to consider time of day and day of the week.
```{r, warning=FALSE, message=FALSE}
# Movie+User+Genre+hr Effect Model
hr_avg <- train_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>% left_join(genre_avg, by='genres') %>% 
  mutate(hr = hour(as_datetime(timestamp))) %>% group_by(hr) %>% 
  summarize(b_hr = mean(rating-mu-b_m-b_u-b_g))
hr_pred <- test_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>% left_join(genre_avg, by='genres') %>% 
  mutate(hr = hour(as_datetime(timestamp))) %>% left_join(hr_avg, by='hr') %>% 
  mutate(pred=mu+b_m+b_u+b_g) %>% pull(pred)
hr_rmse <- RMSE(hr_pred,test_set$rating)
RMSE_result2 <- bind_rows(RMSE_result1, 
                          data_frame(Model="Movie+User+Genre+hr Model", RMSE = hr_rmse ))

# Movie+User+Genre+day Effect Model
day_avg <- train_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>% left_join(genre_avg, by='genres') %>% 
  mutate(day = wday(as_datetime(timestamp))) %>% group_by(day) %>% 
  summarize(b_dy = mean(rating-mu-b_m-b_u-b_g))
day_pred <- test_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>% left_join(genre_avg, by='genres') %>% 
  mutate(day = wday(as_datetime(timestamp))) %>% 
  left_join(day_avg, by='day') %>% mutate(pred=mu+b_m+b_u+b_dy) %>% pull(pred)
day_rmse <- RMSE(day_pred,test_set$rating)
RMSE_result2 <- bind_rows(RMSE_result2, 
                          data_frame(Model="Movie+User+Genre+day Model", RMSE = hr_rmse ))
```

In addition to bias, there is the risk of overfitting and hidden impacts to be overlooked. A regularized effect model was created with a range of numbers to be used to find the best fit.
```{r include=TRUE, message=FALSE}
# Regularized Effects Model
lambdas <- seq(3, 15, 0.5)
RMSEreg <- sapply(lambdas, function(l){
    user_ave <- train_set %>% group_by(userId) %>% 
      summarize(u_i = sum(rating-mu)/(n()+l))
    movie_ave <- train_set %>% left_join(user_ave, by='userId') %>%
      group_by(movieId) %>%  summarize(m_i = sum(rating-mu-u_i)/(n()+l))
    
    pred_rating <- test_set %>% left_join(movie_ave, by= "movieId") %>%
      left_join(user_ave, by='userId') %>% 
      mutate(pred = mu + m_i + u_i ) %>% pull(pred) 
  
    RMSE(pred_rating, test_set$rating)
  })
lambda <- lambdas[which.min(RMSEreg)]
RMSEreg_eff <- RMSEreg[[which.min(RMSEreg)]]
RMSE_result3 <- bind_rows(RMSE_result2, 
                          data_frame(Model="Regularization Model", RMSE = RMSEreg_eff ))
```

The final model to be fitted to this data was Matrix Factorization. This process goes through using algorithms to decompose the user-item interface matrix by looking at two smaller matrices. Prior to doing the Matrix Factorization, the data was reduced to a residual data set which removes all the bias noted previously (genres, userId, and movieId). The variables that need determined to use this is a user index, item index and rating. The recosystem instructions were used to step through this calculation, which provided a good start for initial parameters for fitting. Once the model was trained it was ready to be applied to the test set with the parameters optimally tuned.
```{r include=TRUE}
# Matrix Factorization Model
# Calculating residuals by removing movie, user and genres effect
residual_set <- train_set %>% left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>% 
  mutate(resid = rating - mu - b_m - b_u - b_g)

# Setting data and tuning parameters - used R documentation for parameters
reco <- Reco()
train_reco <- data_memory(user_index=residual_set$userId, 
                          item_index=residual_set$movieId, 
                              rating=residual_set$resid, index1=TRUE)
test_reco <- data_memory(user_index=test_set$userId, 
                         item_index=test_set$movieId, index1=TRUE)

# This step is lengthy and could take 15+ minutes
opts <- reco$tune(train_reco, opts = list(dim = c(10, 20, 30), 
                                          lrate = c(0.1, 0.2),
                                              costp_l1=0, costq_l1=0,
                                              nthread = 1, niter = 10))

# Train the model of reco
reco$train(train_reco, opts = c(opts$min, nthread=1, niter=20))

# Predict results for the test set
reco_pred <- reco$predict(test_reco, out_memory())
preds <- cbind(test_set, reco_pred) %>% 
  left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>% 
  mutate(pred = mu + b_m + b_u + b_g + reco_pred) %>%
  pull(pred)
rmseMF <- RMSE(preds, test_set$rating)
RMSE_result4 <- bind_rows(RMSE_result3, 
                          data_frame(Model="Matrix Factorization", RMSE = rmseMF )) %>% 
  arrange(desc(RMSE))
```

**Results:**

The baseline model was the Naive average model with an RMSE of `r avg_rmse` and was the starting point. From there bias were built in one at a time from movieId to userId and finally genres. A rather decent improvement was seen from the average to the first bias model and again to the second (movieId+userId). Subsequently, the genres bias models only improved the RMSE marginally.
```{r echo=FALSE, fig.align='center'}
kable(RMSE_result1)
```

Different times of the day and days of the week would be expected to play some bias as well but it did not improve the RMSE to an acceptable level (target of 0.8649). Actually, it didn't change the results at all.
```{r echo=FALSE, fig.align='center'}
kable(RMSE_result2)
```

To ensure the models were not over trained or missing some buried relationship unintentionally, the bias effect model was regularized with a lambda. After tuning, the optimum lambda was determined to be `r lambda`. Regularizing the model increased the RMSE unexpectedly. 
```{r, echo=FALSE, fig.align='center'}
plot(lambdas,RMSEreg)
```

```{r include=TRUE}
lambdas[which.min(RMSEreg)] 
```

```{r echo=FALSE, fig.align='center'}
kable(RMSE_result3)
```

Finally, the Matrix Factorization yielded the desired results of RMSE <0.8649. This process takes quite a bit of computing time but yielded an RMSE of `r rmseMF`.
```{r echo=FALSE, fig.align='center'}
kable(RMSE_result4)
```

This model proved to be the best model and was utilized against the validation data set. The original trained reco model was applied to the validation data to form new predictions.
```{r}
# Making sure userId and movieId in validation set are also in the train set
valid <- validation %>% 
  semi_join(train_set, by = "movieId") %>% 
  semi_join(train_set, by = "userId")
valid_reco <- data_memory(user_index=valid$userId, 
                          item_index=valid$movieId, index1=TRUE)
r_pred <- reco$predict(valid_reco, out_memory())
val_preds <- cbind(valid, r_pred) %>% 
  left_join(movie_avg, by='movieId') %>% 
  left_join(user_avg, by='userId') %>%
  left_join(genre_avg, by='genres') %>% 
  mutate(pred = mu + b_m + b_u + b_g + r_pred) %>%
  pull(pred)
rmseFinal <- RMSE(val_preds, valid$rating)
```
Following these steps resulted in a final RMSE of `r rmseFinal`.

**Conclusion:**

After a review of the data and dissecting of the edx dataset the analysis was performed. Initially, there was a step increase but then the other bias and regularization models are improved marginally. The final Matrix Factorization, was the most time consuming but yielded the best results. Within the feature it has algorithms built in already that proved very beneficial in predictiing rating. Through all the models an improvement from `r avg_rmse` with Naive Average prediction to `r rmseFinal` with the final Matrix Factorization with validation data was achieved. If genres were more regulated so that there aren't 797 variations,this could greatly increase predictions. Other factors that could be documented to assist in modeling are movie length and main character.